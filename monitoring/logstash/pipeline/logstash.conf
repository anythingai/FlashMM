# FlashMM Logstash Pipeline Configuration
# Log processing and enrichment for centralized logging

input {
  # Kubernetes container logs via Filebeat
  beats {
    port => 5044
    type => "kubernetes"
  }
  
  # FlashMM application logs
  tcp {
    port => 5000
    type => "flashmm-app"
    codec => json_lines
  }
  
  # Trading engine logs (high priority)
  tcp {
    port => 5001
    type => "trading"
    codec => json_lines
  }
  
  # ML model logs
  tcp {
    port => 5002
    type => "ml"
    codec => json_lines
  }
  
  # Security audit logs
  tcp {
    port => 5003
    type => "security"
    codec => json_lines
  }
  
  # Infrastructure logs
  tcp {
    port => 5004
    type => "infrastructure"
    codec => json_lines
  }
  
  # HTTP logs for API monitoring
  http {
    port => 8080
    type => "http-logs"
    codec => json
  }
}

filter {
  # Common fields for all logs
  mutate {
    add_field => {
      "[@metadata][index_prefix]" => "flashmm-logs"
      "[@metadata][pipeline]" => "flashmm-main"
    }
  }
  
  # Add timestamp if missing
  if ![timestamp] and ![[@timestamp]] {
    mutate {
      add_field => { "timestamp" => "%{+YYYY-MM-dd'T'HH:mm:ss.SSSZ}" }
    }
  }
  
  # Parse container logs from Kubernetes
  if [type] == "kubernetes" {
    # Extract Kubernetes metadata
    if [kubernetes] {
      mutate {
        add_field => {
          "k8s_namespace" => "%{[kubernetes][namespace]}"
          "k8s_pod" => "%{[kubernetes][pod][name]}"
          "k8s_container" => "%{[kubernetes][container][name]}"
          "k8s_node" => "%{[kubernetes][node][name]}"
        }
      }
      
      # Set log level based on container
      if [k8s_container] == "flashmm" {
        mutate { add_field => { "service" => "flashmm-app" } }
      }
      else if [k8s_container] == "redis" {
        mutate { add_field => { "service" => "redis" } }
      }
      else if [k8s_container] == "postgres" {
        mutate { add_field => { "service" => "postgres" } }
      }
      else if [k8s_container] == "influxdb" {
        mutate { add_field => { "service" => "influxdb" } }
      }
    }
    
    # Parse JSON logs if possible
    if [message] =~ /^\{.*\}$/ {
      json {
        source => "message"
        target => "parsed"
      }
      
      # Extract common fields from parsed JSON
      if [parsed][level] {
        mutate { 
          add_field => { "log_level" => "%{[parsed][level]}" }
          uppercase => [ "log_level" ]
        }
      }
      
      if [parsed][msg] {
        mutate { replace => { "message" => "%{[parsed][msg]}" } }
      }
      
      if [parsed][error] {
        mutate { add_field => { "error_message" => "%{[parsed][error]}" } }
      }
    }
  }
  
  # Process FlashMM application logs
  if [type] == "flashmm-app" {
    mutate {
      add_field => { 
        "service" => "flashmm-app"
        "[@metadata][index_prefix]" => "flashmm-app-logs"
      }
    }
    
    # Parse log levels
    if [level] {
      mutate { 
        add_field => { "log_level" => "%{level}" }
        uppercase => [ "log_level" ]
      }
    }
    
    # Extract performance metrics from logs
    if [message] =~ /latency/ {
      grok {
        match => { "message" => "latency: %{NUMBER:latency_ms:float}ms" }
        tag_on_failure => ["_grokparsefailure_latency"]
      }
    }
    
    # Extract trading-related information
    if [message] =~ /(order|trade|position)/ {
      mutate { add_tag => ["trading"] }
      
      grok {
        match => { 
          "message" => [
            "order_id: %{DATA:order_id}",
            "symbol: %{DATA:trading_symbol}",
            "side: %{DATA:order_side}",
            "quantity: %{NUMBER:quantity:float}",
            "price: %{NUMBER:price:float}"
          ]
        }
        tag_on_failure => ["_grokparsefailure_trading"]
      }
    }
    
    # Extract error information
    if [log_level] == "ERROR" {
      if [stack_trace] {
        mutate { add_field => { "error_category" => "application_error" } }
      }
      
      if [message] =~ /connection/ {
        mutate { add_field => { "error_category" => "connection_error" } }
      }
      
      if [message] =~ /(timeout|latency)/ {
        mutate { add_field => { "error_category" => "performance_error" } }
      }
    }
  }
  
  # Process trading engine logs
  if [type] == "trading" {
    mutate {
      add_field => { 
        "service" => "trading-engine"
        "[@metadata][index_prefix]" => "flashmm-trading-logs"
        "priority" => "high"
      }
    }
    
    # Extract trading metrics
    if [pnl] {
      mutate { add_field => { "pnl_value" => "%{pnl}" } }
    }
    
    if [position] {
      mutate { add_field => { "position_size" => "%{position}" } }
    }
    
    # Mark critical trading events
    if [message] =~ /(emergency|halt|stop)/ {
      mutate { add_tag => ["critical", "trading_halt"] }
    }
  }
  
  # Process ML model logs
  if [type] == "ml" {
    mutate {
      add_field => { 
        "service" => "ml-engine"
        "[@metadata][index_prefix]" => "flashmm-ml-logs"
      }
    }
    
    # Extract model performance metrics
    if [accuracy] {
      mutate { add_field => { "model_accuracy" => "%{accuracy}" } }
    }
    
    if [inference_time] {
      mutate { add_field => { "inference_duration" => "%{inference_time}" } }
    }
    
    if [model_name] {
      mutate { add_field => { "ml_model" => "%{model_name}" } }
    }
  }
  
  # Process security logs
  if [type] == "security" {
    mutate {
      add_field => { 
        "service" => "security"
        "[@metadata][index_prefix]" => "flashmm-security-logs"
        "priority" => "high"
      }
    }
    
    # Extract security event information
    if [event_type] {
      mutate { add_field => { "security_event" => "%{event_type}" } }
    }
    
    if [source_ip] {
      mutate { add_field => { "client_ip" => "%{source_ip}" } }
      
      # GeoIP lookup for source IP
      geoip {
        source => "client_ip"
        target => "geoip"
      }
    }
    
    # Mark suspicious activities
    if [message] =~ /(failed.*login|unauthorized|attack|suspicious)/ {
      mutate { add_tag => ["security_threat", "investigation_required"] }
    }
  }
  
  # Process infrastructure logs
  if [type] == "infrastructure" {
    mutate {
      add_field => { 
        "service" => "infrastructure"
        "[@metadata][index_prefix]" => "flashmm-infra-logs"
      }
    }
    
    # Extract resource usage information
    if [message] =~ /cpu/ {
      grok {
        match => { "message" => "cpu: %{NUMBER:cpu_usage:float}%" }
        tag_on_failure => ["_grokparsefailure_cpu"]
      }
    }
    
    if [message] =~ /memory/ {
      grok {
        match => { "message" => "memory: %{NUMBER:memory_usage:float}%" }
        tag_on_failure => ["_grokparsefailure_memory"]
      }
    }
  }
  
  # Common field cleanup and enrichment
  # Remove empty fields
  ruby {
    code => "
      event.to_hash.keys.each { |k| 
        event.remove(k) if event.get(k).nil? || (event.get(k).respond_to?(:empty?) && event.get(k).empty?)
      }
    "
  }
  
  # Add environment information
  if [k8s_namespace] {
    if [k8s_namespace] =~ /dev/ {
      mutate { add_field => { "environment" => "development" } }
    }
    else if [k8s_namespace] =~ /staging/ {
      mutate { add_field => { "environment" => "staging" } }
    }
    else if [k8s_namespace] =~ /prod/ {
      mutate { add_field => { "environment" => "production" } }
    }
  }
  
  # Set default environment if not detected
  if ![environment] {
    mutate { add_field => { "environment" => "unknown" } }
  }
  
  # Add processing timestamp
  mutate {
    add_field => { "processed_at" => "%{+YYYY-MM-dd'T'HH:mm:ss.SSSZ}" }
  }
  
  # Final cleanup
  mutate {
    remove_field => ["parsed", "beat", "agent", "ecs", "input", "host"]
  }
}

output {
  # Debug output (comment out in production)
  # stdout { 
  #   codec => json_lines 
  # }
  
  # Main Elasticsearch output
  elasticsearch {
    hosts => ["${ELASTICSEARCH_HOSTS:elasticsearch:9200}"]
    user => "${ELASTICSEARCH_USERNAME:elastic}"
    password => "${ELASTICSEARCH_PASSWORD}"
    
    # Dynamic index based on service and date
    index => "%{[@metadata][index_prefix]}-%{+YYYY.MM.dd}"
    
    # Index template
    template_name => "flashmm-logs"
    template => "/usr/share/logstash/templates/flashmm-template.json"
    template_overwrite => true
    
    # Document type
    document_type => "_doc"
    
    # Performance settings
    pipeline => "%{[@metadata][pipeline]}"
    retry_on_conflict => 3
    
    # Bulk settings
    bulk_size => 1000
    flush_size => 100
    idle_flush_time => 5
  }
  
  # High priority logs to separate index
  if [priority] == "high" {
    elasticsearch {
      hosts => ["${ELASTICSEARCH_HOSTS:elasticsearch:9200}"]
      user => "${ELASTICSEARCH_USERNAME:elastic}"
      password => "${ELASTICSEARCH_PASSWORD}"
      
      index => "flashmm-priority-logs-%{+YYYY.MM.dd}"
      document_type => "_doc"
    }
  }
  
  # Security logs to SIEM (if configured)
  if [type] == "security" {
    # http {
    #   url => "${SIEM_WEBHOOK_URL}"
    #   http_method => "post"
    #   format => "json"
    # }
  }
  
  # Dead letter queue for failed documents
  if "_jsonparsefailure" in [tags] or "_grokparsefailure" in [tags] {
    elasticsearch {
      hosts => ["${ELASTICSEARCH_HOSTS:elasticsearch:9200}"]
      user => "${ELASTICSEARCH_USERNAME:elastic}"
      password => "${ELASTICSEARCH_PASSWORD}"
      
      index => "flashmm-failed-logs-%{+YYYY.MM.dd}"
      document_type => "_doc"
    }
  }
}